{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from itertools import islice\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import requests\n",
    "import random\n",
    "import networkx as nx\n",
    "from networkx.algorithms.approximation import clique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyvis.network import Network\n",
    "import itertools \n",
    "from scipy import stats\n",
    "\n",
    "print(\"Librerie caricate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data_ids\"\n",
    "pp=pprint.PrettyPrinter()\n",
    "\n",
    "def serialize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    with open(f\"{folder}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}\")\n",
    "\n",
    "def read_json(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf8\") as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data read from path: {path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No data found at path: {path}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth success\n"
     ]
    }
   ],
   "source": [
    "#Fase di autenticazione\n",
    "api_key=\"cvoM8D7hXXxlvBXTM8aH9X2ec\"\n",
    "api_secret=\"Uk2UvH0FJY7KaDzkXYLfgiYkA1OuCwbLlGFPyodB5wcQ5bsItN\"\n",
    "access_token=\"3303466053-OLuExo5KcP8UQCVwZwmyakZs8b91Fpl2lMOUDAe\"\n",
    "access_secret=\"1lMXufu42KN8JvJjYT7c0zI3Q57CkN09BxkNXZuNQ0Dej\"\n",
    "bearer_token=\"AAAAAAAAAAAAAAAAAAAAAHsVJQEAAAAAQ4vYb83r6ueD8QvjJ4Zpx9R7Kbw%3DQuLzsmDYOvpff7lRHGXhNJSOXTFuPyOwLHZv7HPSj9WF34h1E8\"\n",
    "\n",
    "auth=tweepy.OAuthHandler(api_key,api_secret)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    "api=tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "if(api.verify_credentials):\n",
    "    print(\"Auth success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_screen_name=[\"mizzaro\",\"damiano10\",\"miccighel_\",\"eglu81\",\"KevinRoitero\"]\n",
    "users_id=[18932422, 132646210, 15750573, 19659370, 3036907250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scaricare gli id dei followers e dei following dei 5 account principali\n",
    "followers_ids,following_ids={},{}\n",
    "\n",
    "for utente in users_id:\n",
    "    print(utente)\n",
    "    followers_utente,following_utente=[],[]\n",
    "    for item in tweepy.Cursor(\n",
    "        api.followers,\n",
    "        id=utente,\n",
    "        skip_status=True,\n",
    "        include_user_entities=False\n",
    "    ).items():\n",
    "        json_data=item._json\n",
    "        user={\"id\":json_data[\"id\"]}\n",
    "        time.sleep(181)\n",
    "        followers_utente.append(user)\n",
    "\n",
    "    followers_ids[utente]=followers_utente\n",
    "    \n",
    "\n",
    "    for item in tweepy.Cursor(\n",
    "        api.friends,\n",
    "        id=utente,\n",
    "        skip_status=True,\n",
    "        include_user_entities=False\n",
    "    ).items():\n",
    "        json_data=item._json\n",
    "        user={\"id\":json_data[\"id\"]}\n",
    "        time.sleep(181)\n",
    "        following_utente.append(user)\n",
    "\n",
    "    following_ids[utente]=following_utente\n",
    "\n",
    "print(following_ids)\n",
    "\n",
    "#export to json\n",
    "serialize_json(\"data_ids\",\"followers_5_utenti.json\",followers_ids)\n",
    "serialize_json(\"data_ids\",\"following_5_utenti.json\",following_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_ids, following_ids = [], []\n",
    "\n",
    "# punto 2&3\n",
    "followers_ids = read_json(\"data_ids/followers_5_utenti.json\")\n",
    "following_ids = read_json(\"data_ids/following_5_utenti.json\")\n",
    "\n",
    "# 2\n",
    "random_followers_utenti_ids, followers_of_followers_ids = {}, {}\n",
    "\n",
    "for utente in followers_ids:\n",
    "    random_followers_utenti_ids[utente] = random.sample(followers_ids[utente], 5)\n",
    "# result: random_followers_utenti_ids= {\"mizzaro\":[{}{}{}{}{}],\"damiano10\":[{}...}],...}\n",
    "\n",
    "for utente in following_ids:\n",
    "    random_following_utenti_ids[utente] = random.sample(following_ids[utente], 5)\n",
    "# result: random_following_utenti_ids= {\"mizzaro\":[{}{}{}{}{}],\"damiano10\":[{}...}],...}\n",
    "\n",
    "#Download dei followers dei 10 followers degli utenti random \n",
    "for utente in random_followers_utenti_ids:\n",
    "    for f in random_followers_utenti_ids[utente]:\n",
    "        fof = []\n",
    "        print(\"scarico per \" + str(utente))\n",
    "        for item in tweepy.Cursor(\n",
    "                api.followers,\n",
    "                id=f['id'],\n",
    "                skip_status=True,\n",
    "                include_user_entities=False\n",
    "        ).items(10):\n",
    "                time.sleep(10)\n",
    "                json_data = item._json\n",
    "                user = {\"id\": json_data[\"id\"]}\n",
    "                fof.append(user)\n",
    "                print(\"Downloaded: \" + str(user))\n",
    "\n",
    "\n",
    "        followers_of_followers_ids[f['id']] = fof\n",
    "\n",
    "serialize_json(\"data_ids/finale\", \"followers_of_followers.json\", followers_of_followers_ids)\n",
    "\n",
    "#Download dei following dei 10 following degli utenti random \n",
    "for utente in random_following_utenti_ids:\n",
    "    for f in random_following_utenti_ids[utente]:\n",
    "        fof = []\n",
    "        print(\"scarico per \" + str(utente))\n",
    "        for item in tweepy.Cursor(\n",
    "                api.friends,\n",
    "                id=f['id'],\n",
    "                skip_status=True,\n",
    "                include_user_entities=False\n",
    "        ).items(10):\n",
    "            time.sleep(10)\n",
    "            json_data = item._json\n",
    "            user = {\"id\": json_data[\"id\"]}\n",
    "            fof.append(user)\n",
    "            print(\"Downloaded: \" + str(user))\n",
    "\n",
    "        following_of_following_ids[f['id']] = fof\n",
    "\n",
    "serialize_json(\"data_ids/finale\", \"following_of_following.json\", following_of_following_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "random_following_utenti_ids,following_of_following_ids={},{}\n",
    "\n",
    "for utente in following_ids:\n",
    "    random_following_utenti_ids[utente]=random.sample(following_ids[utente],5)\n",
    "\n",
    "try:\n",
    "    for utente in random_following_utenti_ids:\n",
    "        for f in random_following_utenti_ids[utente]:\n",
    "            fof=[]\n",
    "            time.sleep(10)\n",
    "            for item in tweepy.Cursor(\n",
    "                api.following,\n",
    "                id=f,\n",
    "                skip_status=True,\n",
    "                include_user_entities=False\n",
    "            ).items(10):\n",
    "                json_data=item._json\n",
    "                user={\"id\":json_data[\"id\"]}\n",
    "                fof.append(user)\n",
    "\n",
    "            following_of_following_ids[f[\"id\"]]=fof\n",
    "except tweepy.TweepError as error: #Le prime volte mi ha tornato TweepError:Unauthorized, ma ora non più\n",
    "    print(error)\n",
    "serialize_json(\"data_ids/finale\",\"following_of_following.json\",following_of_following_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: data_ids/followers_5_utenti.json\n",
      "Data read from path: data_ids/following_5_utenti.json\n",
      "Data read from path: data_ids/followers_of_followers.json\n",
      "Data read from path: data_ids/following_of_following.json\n",
      "Json caricati\n",
      "3103\n"
     ]
    }
   ],
   "source": [
    "#Unione di tutti gli id rilevati e rimozione dei duplicati\n",
    "\n",
    "#Caricamento json contenente tutti gli ID dei nodi interessati\n",
    "followers_ids=read_json(\"data_ids/followers_5_utenti.json\")\n",
    "following_ids=read_json(\"data_ids/following_5_utenti.json\")\n",
    "followers_of_followers_ids=read_json(\"data_ids/followers_of_followers.json\")\n",
    "following_of_following_ids=read_json(\"data_ids/following_of_following.json\")\n",
    "\n",
    "print(\"Json caricati\")\n",
    "\n",
    "lista_json=[followers_ids,following_ids, followers_of_followers_ids,following_of_following_ids]\n",
    "\n",
    "id_nodi_grafo=[]\n",
    "#Eliminazione duplicati e conteggio di essi.\n",
    "for jsonn in lista_json:\n",
    "    for user in jsonn:\n",
    "        for user_id in jsonn[user]:\n",
    "            if not user_id in id_nodi_grafo:\n",
    "                id_nodi_grafo.append(user_id)\n",
    "\n",
    "print(len(id_nodi_grafo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Scaricare i dati di ogni utente con api.get_user \n",
    "\n",
    "nodes = {}\n",
    "    \n",
    "for user_id in id_nodi_grafo: \n",
    "    #richiamo API GET_USER\n",
    "    utente=api.get_user(id=user_id['id'])._json\n",
    "    node_infos= {} \n",
    "    \n",
    "    node_infos[\"name\"]=utente[\"name\"]\n",
    "    node_infos[\"screen_name\"]=utente[\"screen_name\"]\n",
    "    node_infos[\"location\"]=utente[\"location\"]\n",
    "    node_infos[\"followers_count\"]=utente[\"followers_count\"]\n",
    "    node_infos[\"friends_count\"]=utente[\"friends_count\"]\n",
    "    node_infos[\"statuses_count\"]=utente[\"statuses_count\"]\n",
    "    node_infos[\"created_at\"]=utente[\"created_at\"]\n",
    "   \n",
    "    nodes[user_id['id']] = node_infos\n",
    "print(nodes)\n",
    "print(\"Download dei nodi effettuato\")\n",
    "serialize_json(\"data_ids\", \"nodes_of_twitter_graph.json\", nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: data_ids/api_show_friendship/damiano_edges.json\n",
      "Data read from path: data_ids/api_show_friendship/eglu_edges.json\n",
      "Data read from path: data_ids/api_show_friendship/kevin_edges.json\n",
      "Data read from path: data_ids/api_show_friendship/micch_edges.json\n",
      "Data read from path: data_ids/api_show_friendship/mizzaro_edges.json\n",
      "Data serialized to path: data_ids/edges_of_twitter_graph.json\n"
     ]
    }
   ],
   "source": [
    "#Unione delle relazioni scaricate per ogni utente, necessario per parallelizzare il download\n",
    "damiano_edges=read_json(\"data_ids/api_show_friendship/damiano_edges.json\")\n",
    "eglu_edges=read_json(\"data_ids/api_show_friendship/eglu_edges.json\")\n",
    "kevin_edges=read_json(\"data_ids/api_show_friendship/kevin_edges.json\")\n",
    "micch_edges=read_json(\"data_ids/api_show_friendship/micch_edges.json\")\n",
    "mizzaro_edges=read_json(\"data_ids/api_show_friendship/mizzaro_edges.json\")\n",
    "\n",
    "lista_edges=[damiano_edges,eglu_edges, kevin_edges,micch_edges,mizzaro_edges]\n",
    "\n",
    "main_edges = []\n",
    "#Unione json\n",
    "for main in lista_edges:\n",
    "    for relation in main:\n",
    "        main_edges.append(relation)\n",
    "serialize_json(\"data_ids\", \"edges_of_twitter_graph.json\", main_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from path: data_ids/nodes_of_twitter_graph.json\n",
      "3103\n",
      "1922\n"
     ]
    }
   ],
   "source": [
    "#5. Creazione del grafo\n",
    "twitter_graph = nx.DiGraph(team=\"Loris Parata 144338, Francesco Arzon 142439, Lorenzo Dal Fabbro, Matteo Galvan\")\n",
    "#Aggiunta dei nodi al grafo\n",
    "nodes_of_graph=read_json(\"data_ids/nodes_of_twitter_graph.json\")\n",
    "for ids, node in nodes_of_graph.items():\n",
    "    twitter_graph.add_node(ids,\n",
    "                               id= ids,\n",
    "                               title= node[\"name\"],\n",
    "                               color =\"#ffff00\",\n",
    "                               physics=False,#rende la visualizzazone del grafo più leggera\n",
    "                               name=node['name'],\n",
    "                               screen_name=node['screen_name'],\n",
    "                               location=node['location'],\n",
    "                               followers_count=node[\"followers_count\"],\n",
    "                               following_count=node[\"friends_count\"],\n",
    "                               number_of_twitts=node[\"statuses_count\"],\n",
    "                               data_creazione_profilo=node[\"created_at\"]\n",
    "                              ) \n",
    "#Aggiunta degli archi al grafo, con controllo se è presente nel grafo,\n",
    "for edge in main_edges:\n",
    "    if edge['type'] == 'following':\n",
    "        if twitter_graph.has_node(str(edge['source'])):\n",
    "            twitter_graph.add_edge(str(edge['source']),str(edge['target']))\n",
    "        else:\n",
    "            print(\"Relazione relativa ad un nodo non presente nel grafo\")\n",
    "            \n",
    "print(twitter_graph.number_of_nodes())\n",
    "print(twitter_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Creazione grafo interattivo con pyvis\n",
    "\n",
    "def disegna_grafo(grafo):\n",
    "    nt = Network(\n",
    "        height =\"80%\",\n",
    "        width = \"80%\",\n",
    "        bgcolor=\"#222222\",\n",
    "        font_color=\"white\",\n",
    "        heading= grafo,\n",
    "        directed=True,\n",
    "    )\n",
    "    nt.show_buttons(filter_=None)\n",
    "    nt.from_nx(grafo)\n",
    "   # nt.barnes_hut()\n",
    "    nt.inherit_edge_colors(False)\n",
    "  # nt.set_edge_smooth(\"continuous\") #cambia formato di visualizzazione degli archi\n",
    "    neighbor_map = nt.get_adj_list()\n",
    "    \n",
    "    for node in nt.nodes:\n",
    "            node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "           # print(str(node[\"id\"])+\":\"+ str(node[\"value\"]))\n",
    "         \n",
    "    nt.show(\"grafo.html\")\n",
    "\n",
    "disegna_grafo(twitter_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero nodi del grafo completo\n",
      "3103\n",
      "Numero archi del grafo completo\n",
      "1922\n",
      "Numero di nodi eliminati\n",
      "1424\n",
      "Numero dei nodi del grafo connesso\n",
      "1679\n"
     ]
    }
   ],
   "source": [
    "#Rimozione dei nodi sconnessi per ottenere un sotto grafo connesso\n",
    "print(\"Numero nodi del grafo completo\")\n",
    "print(twitter_graph.number_of_nodes())\n",
    "print(\"Numero archi del grafo completo\")\n",
    "print(twitter_graph.number_of_edges())\n",
    "\n",
    "#Creo una copia del grafo\n",
    "sub_twitter_graph =twitter_graph.copy()\n",
    "\n",
    "nodes_to_delete=[]\n",
    "#Controlla se un nodo ha out_degree = 0,\n",
    "#perchè ci interessano solo i nodi che seguono un account principale\n",
    "for node_id in twitter_graph.nodes():\n",
    "    if(twitter_graph.out_degree[node_id] == 0):\n",
    "        nodes_to_delete.append(node_id)\n",
    "sub_twitter_graph.remove_nodes_from(nodes_to_delete)\n",
    "\n",
    "print(\"Numero di nodi eliminati\")        \n",
    "print(len(nodes_to_delete))\n",
    "print(\"Numero dei nodi del grafo connesso\")\n",
    "print(sub_twitter_graph.number_of_nodes())\n",
    "\n",
    "#disegna_grafo(sub_twitter_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il grafo non è connesso.\n",
      "Il grafo non è bipartito.\n"
     ]
    }
   ],
   "source": [
    "#7 Proprietà del grafo completo\n",
    "undirected_twitter_graph= nx.to_undirected(twitter_graph)\n",
    "\n",
    "if nx.is_connected(undirected_twitter_graph):\n",
    "    print(\"Il grafo è connesso.\")\n",
    "else:\n",
    "    print(\"Il grafo non è connesso.\")\n",
    "    \n",
    "if nx.is_bipartite(undirected_twitter_graph):\n",
    "    print(\"Il grafo è bipartito.\")\n",
    "else:\n",
    "    print(\"Il grafo non è bipartito.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il grafo è connesso.\n",
      "Il grafo non è bipartito.\n"
     ]
    }
   ],
   "source": [
    "#7.1 Proprietà del grafo connesso\n",
    "undirected_sub_twitter_graph= nx.to_undirected(sub_twitter_graph)\n",
    "if nx.is_connected(undirected_sub_twitter_graph):\n",
    "    print(\"Il grafo è connesso.\")\n",
    "else:\n",
    "    print(\"Il grafo non è connesso.\")\n",
    "    \n",
    "if nx.is_bipartite(undirected_sub_twitter_graph):\n",
    "    print(\"Il grafo è bipartito.\")\n",
    "else:\n",
    "    print(\"Il grafo non è bipartito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3036907250', '19659370', '132646210']\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#8 Distanze sul grafo connesso, ma unidirected, perchè non è fortemente connesso\n",
    "centro = nx.center(undirected_sub_twitter_graph)\n",
    "print(centro)\n",
    "diametro = nx.diameter(undirected_sub_twitter_graph)\n",
    "print(diametro)\n",
    "raggio = nx.radius(undirected_sub_twitter_graph)\n",
    "print(raggio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('132646210', 0.21591069996122966), ('19659370', 0.1725877527748354), ('18932422', 0.15167350993349105), ('3036907250', 0.12028088718016337), ('15750573', 0.09360836674428007)]\n",
      "[('94732055', 0.001547318026404474), ('14451127', 0.001547318026404474), ('810893744593584128', 0.001547318026404474), ('125483940', 0.001547318026404474), ('2190533245', 0.001547318026404474)]\n",
      "[('132646210', 0.6243701263224811), ('19659370', 0.23790522256888028), ('18932422', 0.08980472460682286), ('3036907250', 0.03981708773451595), ('15750573', 0.008102838767299813)]\n"
     ]
    }
   ],
   "source": [
    "#9 Misure di centralità sul grafo completo\n",
    "btw_centrality = nx.betweenness_centrality(twitter_graph)\n",
    "#print(btw_centrality)\n",
    "cls_centrality = nx.closeness_centrality(twitter_graph)\n",
    "#print(cls_centralitu)\n",
    "dg_centrality = nx.degree_centrality(twitter_graph)\n",
    "#print(dg_centrality)\n",
    "\n",
    "in_centrality= nx.in_degree_centrality(twitter_graph)\n",
    "#print(in_centrality)\n",
    "out_centrality= nx.out_degree_centrality(twitter_graph)\n",
    "#print(out_centrality)\n",
    "#valori di centralità\n",
    "undirect_centralities=[btw_centrality,cls_centrality,dg_centrality]\n",
    "direct_centralities=[in_centrality,out_centrality]\n",
    "\n",
    "pagerank= nx.pagerank(twitter_graph)\n",
    "pagerank=sorted(pagerank.items(),key=lambda x:x[1],reverse=True)\n",
    "centralities= [btw_centrality,cls_centrality,dg_centrality,in_centrality,out_centrality]\n",
    "print(pagerank[:5])\n",
    "\n",
    "hits= nx.hits(twitter_graph)\n",
    "hits_hubs=hits[0]\n",
    "hits_hub= sorted(hits_hubs.items(),key=lambda x:x[1],reverse=True)\n",
    "print(hits_hub[:5])\n",
    "hits_authorities=hits[1]\n",
    "hits_authorities=sorted(hits_authorities.items(),key=lambda x:x[1],reverse=True)\n",
    "print(hits_authorities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 Misure di centralità sul sotto-grafo\n",
    "btw_centrality = nx.betweenness_centrality(twitter_graph)\n",
    "#print(btw_centrality)\n",
    "cls_centrality = nx.closeness_centrality(twitter_graph)\n",
    "#print(cls_centralitu)\n",
    "dg_centrality = nx.degree_centrality(twitter_graph)\n",
    "#print(dg_centrality)\n",
    "\n",
    "in_centrality= nx.in_degree_centrality(twitter_graph)\n",
    "#print(in_centrality)\n",
    "out_centrality= nx.out_degree_centrality(twitter_graph)\n",
    "#print(out_centrality)\n",
    "#valori di centralità\n",
    "undirect_centralities=[btw_centrality,cls_centrality,dg_centrality]\n",
    "direct_centralities=[in_centrality,out_centrality]\n",
    "\n",
<<<<<<< HEAD
    "pagerank= nx.pagerank(twitter_graph)\n",
=======
    "pagerank= nx.pagerank(sub_twitter_graph)\n",
>>>>>>> parent of bbf6e69... Scrittura quasi completa della relazione
    "pagerank=sorted(pagerank.items(),key=lambda x:x[1],reverse=True)\n",
    "centralities= [btw_centrality,cls_centrality,dg_centrality,in_centrality,out_centrality]\n",
    "print(pagerank[:5])\n",
    "\n",
<<<<<<< HEAD
    "hits= nx.hits(twitter_graph)\n",
=======
    "hits= nx.hits(sub_twitter_graph)\n",
>>>>>>> parent of bbf6e69... Scrittura quasi completa della relazione
    "hits_hubs=hits[0]\n",
    "hits_hub= sorted(hits_hubs.items(),key=lambda x:x[1],reverse=True)\n",
    "print(hits_hub[:5])\n",
    "hits_authorities=hits[1]\n",
    "hits_authorities=sorted(hits_authorities.items(),key=lambda x:x[1],reverse=True)\n",
    "print(hits_authorities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sottografo del nodo KevinRoitero\n",
<<<<<<< HEAD
    "sub_graph_KevinRoitero = nx.ego_graph(twitter_graph, \"3036907250\")\n",
=======
    "sub_graph_KevinRoitero = nx.ego_graph(sub_twitter_graph, \"3036907250\", undirected= True)\n",
>>>>>>> parent of bbf6e69... Scrittura quasi completa della relazione
    "disegna_grafo(sub_graph_KevinRoitero)\n",
    "print(clique.max_clique(sub_graph_KevinRoitero))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 copertua minima degli archi\n",
    "nx.min_edge_cover(undirected_twitter_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 Small-world-ness\n",
    "omega=nx.omega(undirected_twitter_graph, niter=20, nrand=5)\n",
    "print(omega)\n",
    "sigma=nx.sigma(undirected_twitter_graph,niter=20, nrand=5)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 Correlazione di Pearson e di Kendall\n",
    "\n",
    "columns=[\"name\",\"value\"]\n",
    "centralita_df = pd.DataFrame(columns=columns)\n",
    "rows = [[\"btw_centrality\",btw_centrality],[\"cls_centrality\",cls_centrality],[\"dg_centrality\",dg_centrality],[\"in_centrality\",in_centrality],[\"out_centrality\",out_centrality]]\n",
    "#Creazione df contenente tipologia_misura : valore\n",
    "for row in rows:\n",
    "    centralita_df.loc[len(centralita_df)] = row\n",
    "#print(centralita_df)\n",
    "\n",
    "#Creazione df contenente  tipologia_misura_1 , tipologia_misura_2, valore_correlazione_1 , valore_correlazione_2\n",
    "columns=[\"name_1\",\"name_2\",\"pearson\", \"kendall\"]\n",
    "result= pd.DataFrame(columns=columns)\n",
    "\n",
    "for index, param_1 in centralita_df.iterrows():\n",
    "    for index, param_2 in centralita_df.iterrows():\n",
    "        if(param_1[\"name\"] != param_2[\"name\"]):\n",
    "            name_1= param_1[\"name\"]\n",
    "            name_2= param_2[\"name\"]\n",
    "            array_1= np.array(list(param_1[\"value\"].values()), dtype=float)\n",
    "            array_2= np.array(list(param_2[\"value\"].values()), dtype=float)\n",
    "            pearson= stats.pearsonr(array_1,array_2)\n",
    "            kendall= stats.kendalltau(array_1,array_2)\n",
    "            new_row = {\n",
    "            'name_1': name_1,\n",
    "            'name_2': name_2,\n",
    "            'pearson': pearson,\n",
    "            'kendall': kendall\n",
    "                }\n",
    "            result = result.append(new_row, ignore_index=True)\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
